#Make new splits
set.seed(5)
train.index5 <- sample(c(1:dim(wine)[1]), dim(wine)[1]*0.6)
valid.index5 <- setdiff(c(1:dim(wine)[1]), train.index)

train.df5 <- wine[train.index5, ]
valid.df5 <- wine[-train.index5, ]

#regression tree
rtree.fit = rpart(quality ~ volatile.acidity + chlorides + free.sulfur.dioxide
                  + total.sulfur.dioxide + pH + sulphates + alcohol, data = train.df5
                  , method = "anova"
                  , control = rpart.control(minsplit = 30, cp = .001))

#prints complexity parameter table 
printcp(rtree.fit)

# Produces 2 plots. The first plots the r-square (apparent and apparent - from cross-validation) versus the number of splits. The second plots the Relative Error(cross-validation) +/- 1-SE from cross-validation versus the number of splits.
rsq.rpart(rtree.fit)

# plots the cross-validation results across 
plotcp(rtree.fit)

# etailed results including surrogate splits 
summary(rtree.fit)

#ugly tree
plot(rtree.fit, uniform=TRUE, 
     main="Regression Tree for Quality fo Wine")
text(rtree.fit, use.n=TRUE, all=TRUE, cex=.8)

#better printed tree
prp(rtree.fit)
cor(predict(rtree.fit, newdata = valid.df5),valid.df5$quality)^2

#Tried to prune the tree for better accuracy
pruned.rtree.fit<- prune(rtree.fit, cp= rtree.fit$cptable[which.min(rtree.fit$cptable[,"xerror"]),"CP"])
prp(pruned.rtree.fit, main="Pruned Regression Tree for Quality of Wine")
#Doesn't occur
cor(predict(pruned.rtree.fit, newdata = valid.df5),valid.df5$quality)^2

#Tried again
pruned2.rtree.fit = prune(rtree.fit, cp =.01)
prp(pruned2.rtree.fit, main = "Pruned Regression Tree for Quality of Wine")
#Doesn't occur
cor(predict(pruned2.rtree.fit, newdata = valid.df5),valid.df5$quality)^2
